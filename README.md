## V1.0

- 完成爬虫基础架构

## V1.1

- 重建爬虫配置文件格式，可以适配更复杂的网站

## V2.0

- 重新构筑项目结构，之前是通过项目主线程调度各个爬虫线程，整个爬虫项目不由服务器系统控制。这样配置缺点在于只能在开始整个爬虫项目的控制台中控制该项目，一旦退出没有控制台就没有办法监控。
- 从textual制作的伪控制台输出执行状况，改变成用logging输出日志，这样更符合项目整体架构，且相较之前非常容易维护。
- 爬虫之后需要通过corn或者corntab进行调度，定时执行
- 可以通过任意打开的控制台访问指令来控制爬虫的执行

## V2.1

- 将项目轻量化，减少不必要的模块
- 生成部分控制台指令，可以通过控制台实现查询 修改部分配置文件的功能


# 需求分析
## 后端
- 后端不需要管理页面，直接能部署在服务器执行就可以
- 多线程
- 自动化（通过配置需要爬取的内容，直接编写json,不用写入数据库配置）
- 数据保存在数据库（因为可能不是结构化的数据，所以就用mongodb）

### 命名规范
名称 | 中文名称 | 描述
---|---|---
website | 网站 | 主网站
site | 站点 |每个网站可能有多个站点

### 类

名称 |中文名称| 描述
------|---|--------
Crawl() | 单线程类 | 用于创建每个站点爬虫
CrawlPool() | 线程池 | 用于管理所有的站点爬虫


#### Crawl()类
##### 参数
名称 | 中文名称 | 描述 | 样例
---|---|---|---|
url | 地址 | 爬取基本地址 | "http://..."
xpath_dict | xpath字典 | 保存需要爬取数据的xpath |{"title":"...","time":"..."}
internal | 爬取间隔 | 两次爬取之间的时间间隔，以秒为单位 | 10

##### 方法
